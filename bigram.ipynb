{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A basic chatgpt-like language model\n",
    "\n",
    "* Trained on tiny Shakespeare: \n",
    "    \n",
    "    curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "* Character-level\n",
    "\n",
    "* Mostly replicating this repo: https://github.com/karpathy/nanoGPT\n",
    "\n",
    "Paper on transformers: https://arxiv.org/pdf/1706.03762.pdf\n",
    " GPT: Generative Pre-trained Transforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters = 1,115,394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"number of characters = {len(text):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# first 200 characters of data\n",
    "print(text[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "len of unique number of chars = 65\n"
     ]
    }
   ],
   "source": [
    "# unique characters in the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "print(f\"len of unique number of chars = {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining constants\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 39, 50, 47]\n",
      "&ogKB\n",
      "hello ali\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# encode function\n",
    "\n",
    "\n",
    "def encode(s) -> List[int]:\n",
    "    \"\"\"Returns encoded version of input string according to stoi mappings.\n",
    "    \"\"\"\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "# decode function\n",
    "\n",
    "\n",
    "def decode(l) -> str:\n",
    "    \"\"\"Returns decode version of input list of characters according to itos mappings.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "print(encode('hello ali'))\n",
    "print(decode([4, 53, 45, 23, 14]))\n",
    "print(decode(encode('hello ali')))\n",
    "\n",
    "\n",
    "# Note:\n",
    "#   Practically, instead of tokenizing characters, We could character words, or sub-words. \n",
    "#   This would allow for a lot bigger parameter size (instead of 65), and potentially a lot better performance.\n",
    "#   OpenAI tiktoken is a good example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing the entire dataset\n",
    "data = torch.tensor(encode(text))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into train and validation dataset\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "valid_data = data[n:]\n",
    "\n",
    "n, train_data.shape, valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] is used to predict -> 47\n",
      "[18] is used to predict -> 56\n",
      "[18, 47] is used to predict -> 57\n",
      "[18, 47, 56] is used to predict -> 58\n",
      "[18, 47, 56, 57] is used to predict -> 1\n",
      "[18, 47, 56, 57, 58] is used to predict -> 15\n",
      "[18, 47, 56, 57, 58, 1] is used to predict -> 47\n",
      "[18, 47, 56, 57, 58, 1, 15] is used to predict -> 58\n",
      "[18, 47, 56, 57, 58, 1, 15, 47] is used to predict -> 47\n"
     ]
    }
   ],
   "source": [
    "# defining block size: max len of data we use to predict the next character\n",
    "# our final transformer can see one to BLOCK_SIZE number of characters to make the prediction for next\n",
    "# it's max context length for prediction\n",
    "\n",
    "BLOCK_SIZE = 8\n",
    "\n",
    "# example\n",
    "for i in range(BLOCK_SIZE + 1):\n",
    "    print(f\"{data[0:i].tolist()} is used to predict -> {data[i+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is called time dimension\n",
    "data[0:BLOCK_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size (how many obs. we use in each iteration for optimization)\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else valid_data\n",
    "    idx = torch.randint(0, len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n",
    "    x = torch.stack([data[i    : i + BLOCK_SIZE    ] for i in idx])\n",
    "    y = torch.stack([data[i + 1: i + BLOCK_SIZE + 1] for i in idx])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[47, 64, 43, 52, 10,  0, 13, 56],\n",
       "         [52, 43,  6,  0, 20, 39, 58, 46],\n",
       "         [57, 46, 53, 59, 50, 42,  1, 46],\n",
       "         [41, 39, 52,  1, 63, 47, 43, 50]]),\n",
       " tensor([[64, 43, 52, 10,  0, 13, 56, 43],\n",
       "         [43,  6,  0, 20, 39, 58, 46,  1],\n",
       "         [46, 53, 59, 50, 42,  1, 46, 39],\n",
       "         [39, 52,  1, 63, 47, 43, 50, 42]]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb\n",
    "\n",
    "# very important: these are considered independent\n",
    "# meaning that in the example below, we have 32 examples to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = [47] and output = 64\n",
      "input = [47, 64] and output = 43\n",
      "input = [47, 64, 43] and output = 52\n",
      "input = [47, 64, 43, 52] and output = 10\n",
      "input = [47, 64, 43, 52, 10] and output = 0\n",
      "input = [47, 64, 43, 52, 10, 0] and output = 13\n",
      "input = [47, 64, 43, 52, 10, 0, 13] and output = 56\n",
      "input = [47, 64, 43, 52, 10, 0, 13, 56] and output = 43\n",
      "input = [52] and output = 43\n",
      "input = [52, 43] and output = 6\n",
      "input = [52, 43, 6] and output = 0\n",
      "input = [52, 43, 6, 0] and output = 20\n",
      "input = [52, 43, 6, 0, 20] and output = 39\n",
      "input = [52, 43, 6, 0, 20, 39] and output = 58\n",
      "input = [52, 43, 6, 0, 20, 39, 58] and output = 46\n",
      "input = [52, 43, 6, 0, 20, 39, 58, 46] and output = 1\n",
      "input = [57] and output = 46\n",
      "input = [57, 46] and output = 53\n",
      "input = [57, 46, 53] and output = 59\n",
      "input = [57, 46, 53, 59] and output = 50\n",
      "input = [57, 46, 53, 59, 50] and output = 42\n",
      "input = [57, 46, 53, 59, 50, 42] and output = 1\n",
      "input = [57, 46, 53, 59, 50, 42, 1] and output = 46\n",
      "input = [57, 46, 53, 59, 50, 42, 1, 46] and output = 39\n",
      "input = [41] and output = 39\n",
      "input = [41, 39] and output = 52\n",
      "input = [41, 39, 52] and output = 1\n",
      "input = [41, 39, 52, 1] and output = 63\n",
      "input = [41, 39, 52, 1, 63] and output = 47\n",
      "input = [41, 39, 52, 1, 63, 47] and output = 43\n",
      "input = [41, 39, 52, 1, 63, 47, 43] and output = 50\n",
      "input = [41, 39, 52, 1, 63, 47, 43, 50] and output = 42\n"
     ]
    }
   ],
   "source": [
    "for b in range(BATCH_SIZE):\n",
    "    for t in range(BLOCK_SIZE):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"input = {context.tolist()} and output = {target}\")\n",
    "\n",
    "# these are input and output of our model which will be fed into transformer.\n",
    "# the transformer will process these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first example: Bigram model\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # this means given current character, we want to assign a probabiliy (score) to the next one\n",
    "        # previous characters are not impacting our decision\n",
    "        # each encoded character, will de mapped to VOCAB SIZE (usually 65) numbers.\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            self.vocab_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"Defining forward pass.\n",
    "        \"\"\"\n",
    "        # size: Batch (BATCH_SIZE) x Time (BLOCK_SIZE) x Context (VOCAB_SIZE)\n",
    "        logits = self.token_embedding_table(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # resize to (BATCH_SIZE * BLOCK_SIZE) * (VOCAB_SIZE)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)            \n",
    "            targets = targets.view(B * T)        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_new_tokens):\n",
    "        \"Generate an instance from the model, for an input x (encoded) which is from a batch\"\n",
    "        # x has size of (B, T)        \n",
    "        for i in range(max_new_tokens):\n",
    "            # getting predictions\n",
    "            logits, loss = self(x) # loss will be None\n",
    "            # looking at the last character in the time dimension\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            # normalize using softmax to find probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # predicting next character by sampling from probs\n",
    "            x_pred = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            x = torch.cat((x, x_pred), dim=1) # (B, T) -> (B, T + 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramModel(VOCAB_SIZE)\n",
    "m = model.to(device)\n",
    "logits, loss = m(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0411, -0.4302, -0.4431,  ..., -0.5473, -0.3686, -0.0830],\n",
       "          [-1.0915, -0.8579, -0.4286,  ...,  1.1406, -0.3613,  0.3341],\n",
       "          [-1.0915, -0.8579, -0.4286,  ...,  1.1406, -0.3613,  0.3341],\n",
       "          ...,\n",
       "          [-1.1176, -1.1180,  1.6360,  ...,  0.1288,  0.7505, -0.6367],\n",
       "          [-0.4937, -0.3472,  1.0751,  ..., -0.1089, -0.2472,  0.5997],\n",
       "          [-1.0915, -0.8579, -0.4286,  ...,  1.1406, -0.3613,  0.3341]],\n",
       " \n",
       "         [[-0.0411, -0.4302, -0.4431,  ..., -0.5473, -0.3686, -0.0830],\n",
       "          [-1.2101, -0.1748,  0.4630,  ...,  0.8997,  0.5066,  0.1376],\n",
       "          [-1.1176, -1.1180,  1.6360,  ...,  0.1288,  0.7505, -0.6367],\n",
       "          ...,\n",
       "          [ 1.1768, -0.4620,  0.8542,  ...,  0.5255,  0.1622, -0.5660],\n",
       "          [-0.1000, -1.3852,  2.1385,  ...,  1.1285,  0.3761, -0.8725],\n",
       "          [-1.1176, -1.1180,  1.6360,  ...,  0.1288,  0.7505, -0.6367]],\n",
       " \n",
       "         [[-0.1091, -0.5283,  0.6808,  ...,  0.0643,  1.4150,  1.1548],\n",
       "          [-0.7981,  0.9894, -1.5775,  ..., -0.9781, -1.4854, -0.1757],\n",
       "          [-1.1176, -1.1180,  1.6360,  ...,  0.1288,  0.7505, -0.6367],\n",
       "          ...,\n",
       "          [-1.1176, -1.1180,  1.6360,  ...,  0.1288,  0.7505, -0.6367],\n",
       "          [-0.0237,  1.4990,  1.2671,  ..., -2.6499, -2.3506,  0.4163],\n",
       "          [-1.1176, -1.1180,  1.6360,  ...,  0.1288,  0.7505, -0.6367]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0411, -0.4302, -0.4431,  ..., -0.5473, -0.3686, -0.0830],\n",
       "          [-1.0915, -0.8579, -0.4286,  ...,  1.1406, -0.3613,  0.3341],\n",
       "          [-1.1176, -1.1180,  1.6360,  ...,  0.1288,  0.7505, -0.6367],\n",
       "          ...,\n",
       "          [-0.3089,  0.9502, -0.4114,  ...,  0.8843,  0.4689,  0.4301],\n",
       "          [-0.7981,  0.9894, -1.5775,  ..., -0.9781, -1.4854, -0.1757],\n",
       "          [ 0.7080, -0.2576,  0.6910,  ...,  0.8532, -0.4449,  0.3850]],\n",
       " \n",
       "         [[-0.7981,  0.9894, -1.5775,  ..., -0.9781, -1.4854, -0.1757],\n",
       "          [-0.1091, -0.5283,  0.6808,  ...,  0.0643,  1.4150,  1.1548],\n",
       "          [-0.7981,  0.9894, -1.5775,  ..., -0.9781, -1.4854, -0.1757],\n",
       "          ...,\n",
       "          [-1.2101, -0.1748,  0.4630,  ...,  0.8997,  0.5066,  0.1376],\n",
       "          [-0.2263,  0.3291, -0.8471,  ..., -0.1454, -2.7449,  0.6186],\n",
       "          [-0.0237,  1.4990,  1.2671,  ..., -2.6499, -2.3506,  0.4163]],\n",
       " \n",
       "         [[-0.0237,  1.4990,  1.2671,  ..., -2.6499, -2.3506,  0.4163],\n",
       "          [-0.3956,  1.3113,  1.2802,  ...,  1.7323, -0.2458, -0.8445],\n",
       "          [-1.1176, -1.1180,  1.6360,  ...,  0.1288,  0.7505, -0.6367],\n",
       "          ...,\n",
       "          [-1.1176, -1.1180,  1.6360,  ...,  0.1288,  0.7505, -0.6367],\n",
       "          [ 1.1768, -0.4620,  0.8542,  ...,  0.5255,  0.1622, -0.5660],\n",
       "          [-0.1091, -0.5283,  0.6808,  ...,  0.0643,  1.4150,  1.1548]]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can start with this\n",
    "itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwCy.t-eb3v'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tesing generate function (Batch = 1, Time = 1)\n",
    "def generate_text(model, size):\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long, device=device) # initializing with 0, and making sure that we keep the type as long\n",
    "    generated_text = decode(model.generate(x=idx, max_new_tokens=size)[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "generate_text(model=m, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using stochastic gradient, we can use Adam Oprimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3) # learning rate to be 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_ITERS = 200\n",
    "\n",
    "@torch.no_grad() # we don't need any grad calculation here\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval() # setting model into the evaluation mode (will matter if doing batch normalization, etc.)\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            x_eval, y_eval = get_batch(split)\n",
    "            logits, loss = m(x_eval, y_eval)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train() # setting model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0 | train loss = 2.475977897644043 | valid loss = 2.4863996505737305\n",
      "step=1000 | train loss = 2.4659485816955566 | valid loss = 2.4821033477783203\n",
      "step=2000 | train loss = 2.4596831798553467 | valid loss = 2.4804158210754395\n",
      "step=3000 | train loss = 2.459975481033325 | valid loss = 2.479810953140259\n",
      "step=4000 | train loss = 2.4503164291381836 | valid loss = 2.4873416423797607\n",
      "step=5000 | train loss = 2.4522807598114014 | valid loss = 2.476492404937744\n",
      "step=6000 | train loss = 2.458245277404785 | valid loss = 2.482820749282837\n",
      "step=7000 | train loss = 2.445924997329712 | valid loss = 2.488698959350586\n",
      "step=8000 | train loss = 2.458848476409912 | valid loss = 2.4849352836608887\n",
      "step=9000 | train loss = 2.459829330444336 | valid loss = 2.485933780670166\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EVAL_INTERVAL = 1000\n",
    "\n",
    "for step in range(10000):\n",
    "\n",
    "    if (step % EVAL_INTERVAL == 0):\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step={step} | train loss = {losses['train']} | valid loss = {losses['valid']}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # loss calculation\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # optimization\n",
    "    optimizer.zero_grad(set_to_none=True)  # zero grad\n",
    "    loss.backward()\n",
    "    optimizer.step()  # update parameters based on the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JOHilche; h co.\n",
      "Coury?\n",
      "BURor y, crd wo tarreror thindrariathitoth lll dlenjut, t hin t 's ve het\n",
      "LAPUSes.\n",
      "\n",
      "MBar, wiathoffravoue pe best t e thtoufoucive.\n",
      "LOFaishe thy s rigley, geanuk-\n",
      "Whandestharyo w\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model=m, size=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
