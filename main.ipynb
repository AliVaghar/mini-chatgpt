{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A basic chatgpt-like language model\n",
    "\n",
    "* Trained on tiny Shakespeare: \n",
    "    \n",
    "    curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "* Character-level\n",
    "\n",
    "* Mostly replicating this repo: https://github.com/karpathy/nanoGPT\n",
    "\n",
    "Paper on transformers: https://arxiv.org/pdf/1706.03762.pdf\n",
    " GPT: Generative Pre-trained Transforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters = 1,115,394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"number of characters = {len(text):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# first 200 characters of data\n",
    "print(text[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "len of unique number of chars = 65\n"
     ]
    }
   ],
   "source": [
    "# unique characters in the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "print(f\"len of unique number of chars = {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining constants\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 39, 50, 47]\n",
      "&ogKB\n",
      "hello ali\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# encode function\n",
    "\n",
    "\n",
    "def encode(s) -> List[int]:\n",
    "    \"\"\"Returns encoded version of input string according to stoi mappings.\n",
    "    \"\"\"\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "# decode function\n",
    "\n",
    "\n",
    "def decode(l) -> str:\n",
    "    \"\"\"Returns decode version of input list of characters according to itos mappings.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "print(encode('hello ali'))\n",
    "print(decode([4, 53, 45, 23, 14]))\n",
    "print(decode(encode('hello ali')))\n",
    "\n",
    "\n",
    "# Note:\n",
    "#   Practically, instead of tokenizing characters, We could character words, or sub-words. \n",
    "#   This would allow for a lot bigger parameter size (instead of 65), and potentially a lot better performance.\n",
    "#   OpenAI tiktoken is a good example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing the entire dataset\n",
    "data = torch.tensor(encode(text))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into train and validation dataset\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "valid_data = data[n:]\n",
    "\n",
    "n, train_data.shape, valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] is used to predict -> 47\n",
      "[18] is used to predict -> 56\n",
      "[18, 47] is used to predict -> 57\n",
      "[18, 47, 56] is used to predict -> 58\n",
      "[18, 47, 56, 57] is used to predict -> 1\n",
      "[18, 47, 56, 57, 58] is used to predict -> 15\n",
      "[18, 47, 56, 57, 58, 1] is used to predict -> 47\n",
      "[18, 47, 56, 57, 58, 1, 15] is used to predict -> 58\n",
      "[18, 47, 56, 57, 58, 1, 15, 47] is used to predict -> 47\n"
     ]
    }
   ],
   "source": [
    "# defining block size: max len of data we use to predict the next character\n",
    "# our final transformer can see one to BLOCK_SIZE number of characters to make the prediction for next\n",
    "# it's max context length for prediction\n",
    "\n",
    "BLOCK_SIZE = 8\n",
    "\n",
    "# example\n",
    "for i in range(BLOCK_SIZE + 1):\n",
    "    print(f\"{data[0:i].tolist()} is used to predict -> {data[i+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is called time dimension\n",
    "data[0:BLOCK_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size (how many obs. we use in each iteration for optimization)\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else valid_data\n",
    "    idx = torch.randint(0, len(data) - BLOCK_SIZE, (BATCH_SIZE, ))\n",
    "    x = torch.stack([data[i    : i + BLOCK_SIZE    ] for i in idx])\n",
    "    y = torch.stack([data[i + 1: i + BLOCK_SIZE + 1] for i in idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[47, 64, 43, 52, 10,  0, 13, 56],\n",
       "         [52, 43,  6,  0, 20, 39, 58, 46],\n",
       "         [57, 46, 53, 59, 50, 42,  1, 46],\n",
       "         [41, 39, 52,  1, 63, 47, 43, 50]]),\n",
       " tensor([[64, 43, 52, 10,  0, 13, 56, 43],\n",
       "         [43,  6,  0, 20, 39, 58, 46,  1],\n",
       "         [46, 53, 59, 50, 42,  1, 46, 39],\n",
       "         [39, 52,  1, 63, 47, 43, 50, 42]]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb\n",
    "\n",
    "# very important: these are considered independent\n",
    "# meaning that in the example below, we have 32 examples to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = [47] and output = 64\n",
      "input = [47, 64] and output = 43\n",
      "input = [47, 64, 43] and output = 52\n",
      "input = [47, 64, 43, 52] and output = 10\n",
      "input = [47, 64, 43, 52, 10] and output = 0\n",
      "input = [47, 64, 43, 52, 10, 0] and output = 13\n",
      "input = [47, 64, 43, 52, 10, 0, 13] and output = 56\n",
      "input = [47, 64, 43, 52, 10, 0, 13, 56] and output = 43\n",
      "input = [52] and output = 43\n",
      "input = [52, 43] and output = 6\n",
      "input = [52, 43, 6] and output = 0\n",
      "input = [52, 43, 6, 0] and output = 20\n",
      "input = [52, 43, 6, 0, 20] and output = 39\n",
      "input = [52, 43, 6, 0, 20, 39] and output = 58\n",
      "input = [52, 43, 6, 0, 20, 39, 58] and output = 46\n",
      "input = [52, 43, 6, 0, 20, 39, 58, 46] and output = 1\n",
      "input = [57] and output = 46\n",
      "input = [57, 46] and output = 53\n",
      "input = [57, 46, 53] and output = 59\n",
      "input = [57, 46, 53, 59] and output = 50\n",
      "input = [57, 46, 53, 59, 50] and output = 42\n",
      "input = [57, 46, 53, 59, 50, 42] and output = 1\n",
      "input = [57, 46, 53, 59, 50, 42, 1] and output = 46\n",
      "input = [57, 46, 53, 59, 50, 42, 1, 46] and output = 39\n",
      "input = [41] and output = 39\n",
      "input = [41, 39] and output = 52\n",
      "input = [41, 39, 52] and output = 1\n",
      "input = [41, 39, 52, 1] and output = 63\n",
      "input = [41, 39, 52, 1, 63] and output = 47\n",
      "input = [41, 39, 52, 1, 63, 47] and output = 43\n",
      "input = [41, 39, 52, 1, 63, 47, 43] and output = 50\n",
      "input = [41, 39, 52, 1, 63, 47, 43, 50] and output = 42\n"
     ]
    }
   ],
   "source": [
    "for b in range(BATCH_SIZE):\n",
    "    for t in range(BLOCK_SIZE):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"input = {context.tolist()} and output = {target}\")\n",
    "\n",
    "# these are input and output of our model which will be fed into transformer.\n",
    "# the transformer will process these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first example: Bigram model\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # this means given current character, we want to assign a probabiliy (score) to the next one\n",
    "        # previous characters are not impacting our decision\n",
    "        self.token_embedding_table = nn.Embedding(self.vocab_size, self.vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        \"\"\"Defining forward pass.\n",
    "        \"\"\"\n",
    "        # size: Batch (BATCH_SIZE) x Time (BLOCK_SIZE) x Channel (VOCAB_SIZE)\n",
    "        logits = self.token_embedding_table(x)\n",
    "\n",
    "        # nll, expects (B, C, T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        print(logits.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BigramModel(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 65], got [4, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/alighasemivaghar/Documents/myCodes/deep-learning/mini-chatgpt/main.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alighasemivaghar/Documents/myCodes/deep-learning/mini-chatgpt/main.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m m(xb, yb)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ds_dev/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/alighasemivaghar/Documents/myCodes/deep-learning/mini-chatgpt/main.ipynb Cell 25\u001b[0m in \u001b[0;36mBigramModel.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alighasemivaghar/Documents/myCodes/deep-learning/mini-chatgpt/main.ipynb#X62sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# size: Batch (BATCH_SIZE) x Time (BLOCK_SIZE) x Context (VOCAB_SIZE)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alighasemivaghar/Documents/myCodes/deep-learning/mini-chatgpt/main.ipynb#X62sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding_table(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alighasemivaghar/Documents/myCodes/deep-learning/mini-chatgpt/main.ipynb#X62sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(logits, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alighasemivaghar/Documents/myCodes/deep-learning/mini-chatgpt/main.ipynb#X62sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(logits\u001b[39m.\u001b[39mshape, targets\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ds_dev/lib/python3.8/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 65], got [4, 8]"
     ]
    }
   ],
   "source": [
    "m(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[64, 43, 52, 10,  0, 13, 56, 43],\n",
       "        [43,  6,  0, 20, 39, 58, 46,  1],\n",
       "        [46, 53, 59, 50, 42,  1, 46, 39],\n",
       "        [39, 52,  1, 63, 47, 43, 50, 42]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.token_embedding_table(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[47, 64, 43, 52, 10,  0, 13, 56],\n",
       "        [52, 43,  6,  0, 20, 39, 58, 46],\n",
       "        [57, 46, 53, 59, 50, 42,  1, 46],\n",
       "        [41, 39, 52,  1, 63, 47, 43, 50]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(65, 65)\n"
     ]
    }
   ],
   "source": [
    "print(m.token_embedding_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([65, 65]),\n",
       " Parameter containing:\n",
       " tensor([[ 0.7761, -0.0236,  0.7694,  ...,  0.8682,  0.6140,  0.3228],\n",
       "         [-0.1667,  0.2752, -0.6079,  ..., -0.9631, -0.1148,  0.6909],\n",
       "         [ 0.4106,  0.5041,  1.2860,  ..., -1.3456, -0.7963, -1.2198],\n",
       "         ...,\n",
       "         [-0.7582, -1.8711,  0.7141,  ..., -0.5707, -0.4843, -0.0299],\n",
       "         [ 1.1441,  1.4002, -0.5325,  ...,  0.6729,  0.9766, -2.2641],\n",
       "         [-0.4311, -0.2802,  1.9070,  ...,  1.5897, -1.1810, -0.7791]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.token_embedding_table.weight.shape, m.token_embedding_table.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
